{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# PARTIE Formulaire "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\soura\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "import ipywidgets as widgets\n",
    "from IPython.display import display\n",
    "import ipywidgets as widgets\n",
    "\n",
    "\n",
    "import praw\n",
    "from classes.Document import Document \n",
    "from classes.Author import Author\n",
    "from classes.Corpus import Corpus\n",
    "from classes.Document import RedditDocument, ArxivDocument\n",
    "import datetime \n",
    "import pandas as pd \n",
    "import urllib.request\n",
    "import xmltodict\n",
    "import numpy as np\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "#=========================\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Création du corpus\n",
    "mon_corpus = Corpus(nom=\"Corpus_article\")\n",
    "# Chargez le corpus depuis un fichier CSV la méthode load prend en parametre le chemin de votre corpus\n",
    "#mon_corpus.load('corpus.csv')\n",
    "#del(Document)\n",
    "#del(mon_corpus)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e71183d22ba1407a8dfbda112fd21029",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Text(value='Football', description='Thème Reddit:')"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c9612e60af7247deb8e36aa7e0374244",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "IntSlider(value=10, description='Limite Reddit:', min=1)"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d7ed135f848a4c6ab10508194b2e253f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Text(value='_jjmAvQmLvyPWeH7mSTYrw', description='ID Client:')"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a903383665ca4558ad93d87b80466e24",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Text(value='j7vF0wxXN9VuvOrIri3dt3W4fSvH4w', description='Secret Client:')"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "39decba397014dd19de3af745f983772",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Text(value='td3', description='Utilisateur:')"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "355e8f83d7384846a0c4bedb997f7fe7",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Text(value='foot', description='Thème Arxiv:')"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e2487e87fb0743fdb28d59cb26783d85",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "IntSlider(value=10, description='Limite Arxiv:', min=1)"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "71d0417e25b5435da19b480cd6219dcf",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Button(description='Récuperer les données', style=ButtonStyle())"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d8ebf19b796b474da2df02399b654d75",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Output()"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2b61a8ca25454984b72ede9775fd1825",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Button(description='Afficher le Corpus', style=ButtonStyle())"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9fd3da9d821a4550aecaba957a91a8ed",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Button(description='Charger le Corpus', style=ButtonStyle())"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f170aedcac974bb3ac48b3e9335c8bb1",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Button(description='Enregistrer le Corpus', style=ButtonStyle())"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "It appears that you are using PRAW in an asynchronous environment.\n",
      "It is strongly recommended to use Async PRAW: https://asyncpraw.readthedocs.io.\n",
      "See https://praw.readthedocs.io/en/latest/getting_started/multiple_instances.html#discord-bots-and-asynchronous-environments for more info.\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Nombre de documents avant le nettoyage : 20\n",
      "Nombre de documents après le nettoyage : 16\n"
     ]
    }
   ],
   "source": [
    "import ipywidgets as widgets\n",
    "from IPython.display import display\n",
    "# Créer des widgets pour le formulaire Arxiv\n",
    "theme_arxiv_input = widgets.Text(value='foot', description='Thème Arxiv:')\n",
    "limit_arxiv_input = widgets.IntSlider(value=10, min=1, max=100, description='Limite Arxiv:')\n",
    "\n",
    "# Créer des widgets pour le formulaire Reddit\n",
    "theme_reddit_input = widgets.Text(value='Football', description='Thème Reddit:')\n",
    "limit_reddit_input = widgets.IntSlider(value=10, min=1, max=100, description='Limite Reddit:')\n",
    "idclient_input = widgets.Text(value='_jjmAvQmLvyPWeH7mSTYrw', description='ID Client:')\n",
    "secretclient_input = widgets.Text(value='j7vF0wxXN9VuvOrIri3dt3W4fSvH4w', description='Secret Client:')\n",
    "user_input = widgets.Text(value='td3', description='Utilisateur:')\n",
    "\n",
    "# Créer des widgets pour les boutons Afficher, Charger et Enregistrer le corpus\n",
    "button_afficher = widgets.Button(description='Afficher le Corpus')\n",
    "button_charger = widgets.Button(description='Charger le Corpus')\n",
    "button_enregistrer = widgets.Button(description='Enregistrer le Corpus')\n",
    "\n",
    "# Créer des widgets pour afficher les résultats\n",
    "result_output = widgets.Output()\n",
    "\n",
    "# Créer un bouton pour charger les données\n",
    "load_data_button = widgets.Button(description='Récuperer les données')\n",
    "\n",
    "\n",
    "# Fonction appelée lors du clic sur le bouton et qui va recuperer les données Arxiv et Reddit \n",
    "def load_data(button):\n",
    "    #PArtie REDDIT\n",
    "    global collection  # Assurez-vous de déclarer collection comme une variable globale\n",
    "    theme = theme_reddit_input.value\n",
    "    limit = limit_reddit_input.value\n",
    "    idclient = idclient_input.value\n",
    "    secretclient = secretclient_input.value\n",
    "    user = user_input.value\n",
    "\n",
    "    reddit = praw.Reddit(client_id=idclient, client_secret=secretclient, user_agent=user)\n",
    "    subr = reddit.subreddit(theme)\n",
    "\n",
    "    collection = []\n",
    "\n",
    "    for doc in subr.controversial(limit=limit):\n",
    "        titre = doc.title.replace(\"\\n\", '')\n",
    "        auteur = str(doc.author)\n",
    "        date = datetime.datetime.fromtimestamp(doc.created).strftime(\"%Y/%m/%d\")\n",
    "        url = \"https://www.reddit.com/\" + doc.permalink\n",
    "        texte = doc.selftext.replace(\"\\n\", \"\")\n",
    "\n",
    "        doc_class = Document(titre, auteur, date, url, texte)\n",
    "        collection.append(doc_class)\n",
    "        \n",
    "# ====================================================Partie Arxiv : \n",
    "#==========================chargement des données Arxiv en instanciant un objet docyment\n",
    "    theme_arxiv = theme_arxiv_input.value #Foot\n",
    "    limit_arxiv = limit_arxiv_input.value\n",
    "    url = f'http://export.arxiv.org/api/query?search_query=all:{theme_arxiv}&start=0&max_results={limit_arxiv}'\n",
    "    url_read = urllib.request.urlopen(url).read()\n",
    "   \n",
    "\n",
    "    # url_read est un \"byte stream\" qui a besoin d'être décodé\n",
    "    data =  url_read.decode()\n",
    "    dico = xmltodict.parse(data) #xmltodict permet d'obtenir un objet ~JSON\n",
    "    docs = dico['feed']['entry']\n",
    "    for doc in docs:\n",
    "        titre = doc[\"title\"].replace('\\n', '')  # On enlève les retours à la ligne\n",
    "        try:\n",
    "            authors = \", \".join([a[\"name\"] for a in doc[\"author\"]])  # On fait une liste d'auteurs, séparés par une virgule\n",
    "        except:\n",
    "            authors = doc[\"author\"][\"name\"]  # Si l'auteur est seul, pas besoin de liste\n",
    "        summary = doc[\"summary\"].replace(\"\\n\", \"\")  # On enlève les retours à la ligne\n",
    "        date = datetime.datetime.strptime(doc[\"published\"], \"%Y-%m-%dT%H:%M:%SZ\").strftime(\"%Y/%m/%d\")  # Formatage de la date en année/mois/jour avec librairie datetime\n",
    "\n",
    "        doc_class = Document(titre, authors, date, doc[\"id\"], summary)  # Création du Document\n",
    "        collection.append(doc_class)  # Ajout du Document à la liste.\n",
    "        \n",
    "    # Enlever les doublons en se basant sur tout le contenu\n",
    "    seen_documents = set()\n",
    "    unique_collection = []\n",
    "\n",
    "    for doc in collection:\n",
    "        doc_tuple = (doc.titre, doc.auteur, doc.date, doc.texte, doc.type)\n",
    "        if doc_tuple not in seen_documents:\n",
    "            seen_documents.add(doc_tuple)\n",
    "            unique_collection.append(doc)\n",
    "\n",
    "    # Garder uniquement les documents avec plus de 200 caractères dans le texte\n",
    "    filtered_collection = [doc for doc in unique_collection if len(doc.texte) > 200]\n",
    "\n",
    "    # Afficher le nombre de documents avant et après le nettoyage\n",
    "    print(f\"Nombre de documents avant le nettoyage : {len(collection)}\")\n",
    "    print(f\"Nombre de documents après le nettoyage : {len(filtered_collection)}\")\n",
    "\n",
    "    # Réaffecter la liste filtrée à votre collection\n",
    "    collection = filtered_collection\n",
    "\n",
    "    #Rempli le corpus \n",
    "    for doc in collection:\n",
    "        if \"reddit\" in doc.url.lower():\n",
    "            doc.type = \"Reddit\"\n",
    "        else:\n",
    "            doc.type = \"Arxiv\"\n",
    "        mon_corpus.add(doc)\n",
    "    \n",
    "        \n",
    "    with result_output:\n",
    "        result_output.clear_output(wait=True)\n",
    "        print(\"Données chargées avec succès.\")\n",
    "            \n",
    "\n",
    "\n",
    "#=================================== Creation du corpus======================= \n",
    "## =======================================================================\n",
    "# Fonctions associées aux boutons\n",
    "def afficher_corpus(button):\n",
    "    # Logique pour afficher le corpus\n",
    "    with result_output:\n",
    "        result_output.clear_output(wait=True)\n",
    "        mon_corpus.show() \n",
    "\n",
    "def charger_corpus(button):\n",
    "    mon_corpus.load(\"corpus1.csv\") #Chemin n'hésitez à le modifier \n",
    "    with result_output:\n",
    "        result_output.clear_output(wait=True)\n",
    "        print(\"Corpus chargé avec succès Vers le chemin indiqué.\")\n",
    "\n",
    "def enregistrer_corpus(button):\n",
    "    mon_corpus.save(\"corpus1.csv\")\n",
    "    with result_output:\n",
    "        result_output.clear_output(wait=True)\n",
    "        print(\"Corpus enregistré avec succès.\")\n",
    "\n",
    "\n",
    "load_data_button.on_click(load_data)#bouton pour recuperer les données\n",
    "button_afficher.on_click(afficher_corpus)#bouton pour afficher le corpus\n",
    "button_charger.on_click(charger_corpus)#bouton pour charger le corpus depuis l'ordinateur\n",
    "button_enregistrer.on_click(enregistrer_corpus)#bouton pour enregistrer le corpus dans lordinateur \n",
    "\n",
    "# Affichage des widgets\n",
    "display(\n",
    "    theme_reddit_input, limit_reddit_input, idclient_input, secretclient_input, user_input,\n",
    "    theme_arxiv_input, limit_arxiv_input, load_data_button, result_output,\n",
    "    button_afficher, button_charger, button_enregistrer\n",
    ")\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Formulaires recherches DIFFERENTES METHODES"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "## Methode 1 : SEARCH \n",
    " Ici on va juste afficher les passages qu'on va rencontrer les mots clés dans les differents documents de notre corpus "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ff287f01bcf84879bbecb24f94bdf0c2",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Text(value='', description='Mots-clés:', placeholder='Entrez vos mots-clés')"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7d87207c4e6b43189e34b215b60de6e0",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Button(description='Rechercher', style=ButtonStyle())"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import ipywidgets as widgets\n",
    "from IPython.display import display\n",
    "\n",
    "id2doc = mon_corpus.get_id2doc()\n",
    "# Fonction pour gérer la recherche\n",
    "def effectuer_recherche(b):\n",
    "    mots_clefs = champ_recherche.value\n",
    "    passages = mon_corpus.search(mots_clefs)\n",
    "    \n",
    "    # Afficher les résultats\n",
    "    for doc_id, passage in passages.items():\n",
    "        document = id2doc[doc_id]\n",
    "        print(f\"Document ID: {doc_id}\")\n",
    "        #print(f\"Auteur: {document.auteur}\")\n",
    "        print(f\"Passage: {passage}\")\n",
    "        print(\"\\n\")\n",
    "\n",
    "# Champ de recherche\n",
    "champ_recherche = widgets.Text(description=\"Mots-clés:\", placeholder=\"Entrez vos mots-clés\")\n",
    "\n",
    "# Bouton pour effectuer la recherche\n",
    "bouton_recherche = widgets.Button(description=\"Rechercher\")\n",
    "bouton_recherche.on_click(effectuer_recherche)\n",
    "\n",
    "# Afficher le formulaire\n",
    "display(champ_recherche, bouton_recherche)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Methode 2 : matrice tf_idf avec mesure de similarité cos \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "cd2cd5982615423e9553b99da6f6a392",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Text(value='', description='Mots-clés:', placeholder='Entrez vos mots-clés')"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "258fa26f2d2a4b95a8714d59b89ad998",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "IntText(value=5, description='Top N:')"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "96378a3b63ec4cc894f1b02cff1168ef",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Button(description='Rechercher avec Cosinus', style=ButtonStyle())"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{1: Titre : Is it just me or is Reece James INCREDIBLY overrated?\tAuteur : Elon-Mustget-thatass\tDate : 2023/11/25\tURL : https://www.reddit.com//r/football/comments/183paq1/is_it_just_me_or_is_reece_james_incredibly/\tTexte : I genuinely think this guy has some incredible PR with the help of Chelsea fans and the media. This dude has straight up been pants since march of 2022 and is somehow still in the “best rb itw” convo “when fit/available”. Is built like a gym devotee yet has the endurance and stamina of a wet shoe box, puts in a good attacking performance every 3 to 5 BUSINESS weeks (not even an exaggeration) and defensively, well, ask Anthony F*CKING Gordon who boned him and got him sent to the showers, probably for the better.\tType : Reddit\t, 2: Titre : Messi was NOT the best player last season and/or year\tAuteur : thesadhra\tDate : 2023/10/26\tURL : https://www.reddit.com//r/football/comments/17gihhq/messi_was_not_the_best_player_last_season_andor/\tTexte : I know the Messi defence brigade will come out of their holes to justify this decision but honestly at this point I don’t care. Messi obviously one of the best players of all time but in no way was Messi better than Haaland last season. Outside of the WC Messi was outshined by Mbappe within the same league and he got dogwalked by Bayern in the UCL. If Messi went to the premier league and in his very first season broke the prem goal record, was top scorer in the UCL and went ahead and won the clubs first ever treble then he would have won the Ballon D’or regardless of whatever happened in the WC. The people calling for him to win the award now will still be wanting him to win it then. Let’s face it the award is massively biased and is nothing more than a popularity contest. Messi and others have robbed people in the past and they’ll continue to do so in the future.\tType : Reddit\t, 3: Titre : England unfairly play at home?\tAuteur : Regretto_hardbass\tDate : 2021/07/07\tURL : https://www.reddit.com//r/football/comments/ofsryf/england_unfairly_play_at_home/\tTexte : How unfair is it from UEFA to have England play 6 out 7 games AT HOME???? It is just unbelievable especially now that they're in the final and have all the fans at Wembley..Edit: Okay I see a lot of ignorance, so I will update you on the situation, so there is a little situation going on called pandemic and unlike previous years you can't go to wembley if you are a foreigner and want to support your team, understood?Re edit: I thought there would have been a more adult conversation but it is just a lot of mad english fans not answering my question, oh well\tType : Reddit\t, 4: Titre : Why are mainstream sports networks misrepresenting Mudryk’s transfer fee ?\tAuteur : wilmo1247\tDate : 2024/01/04\tURL : https://www.reddit.com//r/football/comments/18ybrbf/why_are_mainstream_sports_networks/\tTexte : He cost an initial 62m with another 26m depending on Chelsea winning Multiple PL and CL and Mudryk winning ballon D’or If shakhtar get all the add ons from Chelsea its not only a good deal but a bargain especially when you think Newcastle paid 60m 4 Gordon and Arsenal paid 65 for havertz\tType : Reddit\t, 5: Titre : VAR on its own kills the excitement of goals for me\tAuteur : spinebreak01\tDate : 2022/12/05\tURL : https://www.reddit.com//r/football/comments/zd1jum/var_on_its_own_kills_the_excitement_of_goals_for/\tTexte : Back in the days I would be so happy for my team to score, yelling in front of the TV and going crazy, this is how much I loved the sport. VAR nowadays makes these cathartic feelings non-existent for me, as I have a constant phobia of VAR taking the goal away for offside or some other reason. Is that a problem you guys dealing with too, or the burnout is only real for me? I have seen so many goals taken away that I am always reluctant to feel the joy before making sure VAR is allowing it and that feeling simply hits different.\tType : Reddit\t, 6: Titre : VAR is doing just fine. You just freak out when it’s against your team.\tAuteur : carbust20\tDate : 2023/11/09\tURL : https://www.reddit.com//r/football/comments/17rapci/var_is_doing_just_fine_you_just_freak_out_when/\tTexte : It probably gets 95% of the calls right. FIFA says 99.3% but I don’t think so. Still, if it didn’t exist everyone would be crying to implement technology like they did before.  Think of all the goals/no goals that have been rightfully overturned. Even if there are some horrible calls sometimes, the good calls outweigh the bad ones.\tType : Reddit\t, 7: Titre : Dynamic foot morphology explained through 4D scanning and shape modeling\tAuteur : Abhishektha Boppana, Allison P. Anderson\tDate : 2020/07/21\tURL : http://arxiv.org/abs/2007.11077v1\tTexte : A detailed understanding of foot morphology can enable the design of morecomfortable and better fitting footwear. However, foot morphology varies widelywithin the population, and changes dynamically during the loading of stancephase. This study presents a parametric statistical shape model from 4D footscans to capture both the inter- and intra-individual variability in footmorphology. Thirty subjects walked on a treadmill while 4D scans of their rightfoot were taken at 90 frames-per-second during stance phase. Each subject'sheight, weight, foot length, foot width, arch length, and sex were alsorecorded. The 4D scans were all registered to a common high-quality foot scan,and a principal component analysis was done on all processed 4D scans.Elastic-net linear regression models were built to predict the principalcomponent scores, which were then inverse transformed into 4D scans. The bestperforming model was selected with leave-one-out cross-validation. The chosenmodel was predicts foot morphology across stance phase with a root-mean squarederror of 5.2 +/- 2.0 mm. This study shows that statistical shape modeling canbe used to predict dynamic changes in foot morphology across the population.The model can be used to investigate and improve foot-footwear interaction,allowing for better fitting and more comfortable footwear.\tType : Arxiv\t, 8: Titre : Mobile Apps for Foot Measurement: A Scoping Review\tAuteur : Muhammad Ashad Kabir, Sowmen Rahman, Mohammad Mainul Islam, Sayed Ahmed, Craig Laird\tDate : 2020/09/09\tURL : http://arxiv.org/abs/2009.04198v1\tTexte : With the proliferation of smart phone, a major growth in the use of appsrelated to the health category, specifically those concerned with foot healthcan be observed. Although new, these apps are being used practically forscanning feet with an aim to providing accurate information about variousproperties of the human foot. With the availability of many 'foot scanning andmeasuring apps' in the app stores, the need for an evaluation system for suchapps can be deemed necessary as little information regarding the evidence-basedquality of these apps is available. To characterize the assessment ofmeasurement techniques and essential software quality characteristics of mobilefoot measuring apps, and determine their effectiveness for potential use ascommercial professional tools for foot care health professionals such aspedorthists, podiatrists, orthotists and so on, to assist in measuring foot forcustom shoes, and for individuals to enhance the awareness of foot health andhygiene and prevention of foot-related problems. An electronic search acrossAndroid and iOS app stores was conducted between July 2020 and August 2020 forapps related to foot measurement. Mobile apps with stated goals of footmeasurement and general foot health were identified and selected by threeindependent raters and discrepancies regarding the selected apps were resolvedvia a fourth rater. Evaluation inferences found all apps failing to meet evenhalf of the measurement-specific criteria required for the proper manufacturingof custom-made footwear. 23% (6/26) of apps were found to utilize eitherexternal scanners or advanced algorithms to reconstruct 3D models of user footthat can possibly be used for ordering custom-made footwear and medical castsfor fitting irregular foot sizes and shapes. Overall, current apps for footmeasurement do not follow any specific guidelines for measurement purposes.\tType : Arxiv\t, 9: Titre : f2IMU-R: Pedestrian Navigation by Low-cost Foot-Mounted Dual IMUs and  Inter-foot Ranging\tAuteur : Maoran Zhu, Yuanxin Wu, Shitu Luo\tDate : 2020/12/08\tURL : http://arxiv.org/abs/2012.04143v1\tTexte : Foot-mounted inertial sensors become popular in many indoor or GPS-deniedapplications, including but not limited to medical monitoring, gait analysis,soldier and first responder positioning. However, the foot-mounted inertialnavigation relies largely on the aid of Zero Velocity Update (ZUPT) and hasencountered inherent problems such as heading drift. This paper implements apedestrian navigation system based on dual foot-mounted low-cost inertialmeasurement units (IMU) and inter-foot ultrasonic ranging. The observabilityanalysis of the system is performed to investigate the roles of the ZUPTmeasurement and the foot-to-foot ranging measurement in improving the stateestimability. A Kalman-based estimation algorithm is mechanized in the Earthframe, rather than in the common local-level frame, which is found to beeffective in depressing the linearization error in Kalman filtering. Anellipsoid constraint in the Earth frame is also proposed to further restrictthe height drift. Simulation and real field experiments show that the proposedmethod has better robustness and positioning accuracy (about 0.1-0.2% travelleddistance) than the traditional pedestrian navigation schemes do.\tType : Arxiv\t, 10: Titre : Software Based Higher Order Structural Foot Abnormality Detection Using  Image Processing\tAuteur : Arnesh Sen, Kaustav Sen, Jayoti Das\tDate : 2019/04/11\tURL : http://arxiv.org/abs/1904.05651v1\tTexte : The entire movement of human body undergoes through a periodic process namedGait Cycle. The structure of human foot is the key element to complete thecycle successfully. Abnormality of this foot structure is an alarming form ofcongenital disorder which results a classification based on the geometry of thehuman foot print image. Image processing is one of the most efficient way todetermine a number of footprint parameter to detect the severeness of disorder.This paper aims to detect the Flatfoot and High Arch foot abnormalities usingone of the footprint parameters named Modified Brucken Index by biomedicalimage processing.\tType : Arxiv\t, 11: Titre : Natural gaits of the non-pathological flat foot and high-arched foot\tAuteur : Yifang Fan, Yubo Fan, Zhiyu Li, Changsheng Lv, Donglin Luo\tDate : 2010/12/17\tURL : http://arxiv.org/abs/1012.3816v2\tTexte : There has been a controversy as to whether or not the non-pathological flatfoot and high-arched foot have an effect on human walking activities. The 3Dfoot scanning system was employed to obtain static footprints from subjectsadopting a half-weight-bearing stance. Based upon their footprints, thesubjects were divided into two groups: the flat-footed and the high-arched. Theplantar pressure measurement system was used to measure and record thesubjects' successive natural gaits. Two indices were proposed: distribution ofvertical ground reaction force (VGRF) of plantar and the rate of the footprintareas. Using these two indices to compare the natural gaits of the two subjectgroups, we found that (1) in stance phase, there is a significant difference(p<0.01) in the distributions of VGRF of plantar; (2) in a stride cycle, thereis also a significant difference (p<0.01) in the rates of the footprint areas.Our analysis suggests that when walking, the VGRF of the plantar brings greatermuscle tension to the flat-footed while a smaller rate of the footprint areasbrings greater stability to the high-arched.\tType : Arxiv\t, 12: Titre : Translation-Invariant Representation for Cumulative Foot Pressure Images\tAuteur : Shuai Zheng, Kaiqi Huang, Tieniu Tan\tDate : 2010/10/26\tURL : http://arxiv.org/abs/1010.5426v1\tTexte : Human can be distinguished by different limb movements and unique groundreaction force. Cumulative foot pressure image is a 2-D cumulative groundreaction force during one gait cycle. Although it contains pressure spatialdistribution information and pressure temporal distribution information, itsuffers from several problems including different shoes and noise, when puttingit into practice as a new biometric for pedestrian identification. In thispaper, we propose a hierarchical translation-invariant representation forcumulative foot pressure images, inspired by the success of Convolutional deepbelief network for digital classification. Key contribution in our approach isdiscriminative hierarchical sparse coding scheme which helps to learn usefuldiscriminative high-level visual features. Based on the feature representationof cumulative foot pressure images, we develop a pedestrian recognition systemwhich is invariant to three different shoes and slight local shape change.Experiments are conducted on a proposed open dataset that contains more than2800 cumulative foot pressure images from 118 subjects. Evaluations suggest theeffectiveness of the proposed method and the potential of cumulative footpressure images as a biometric.\tType : Arxiv\t, 13: Titre : Re-weighting of somatosensory inputs from the foot and the ankle for  controlling posture during quiet standing following trunk extensor muscles  fatigue\tAuteur : Nicolas Vuillerme, Nicolas Pinsault\tDate : 2008/02/13\tURL : http://arxiv.org/abs/0802.1907v1\tTexte : The present study focused on the effects of trunk extensor muscles fatigue onpostural control during quiet standing under different somatosensory conditionsfrom the foot and the ankle. With this aim, 20 young healthy adults were askedto stand as immobile as possible in two conditions of No fatigue and Fatigue oftrunk extensor muscles. In Experiment 1 (n = 10), somatosensation from the footand the ankle was degraded by standing on a foam surface. In Experiment 2 (n =10), somatosensation from the foot and ankle was facilitated through theincreased cutaneous feedback at the foot and ankle provided by strips ofathletic tape applied across both ankle joints. The centre of foot pressuredisplacements (CoP) were recorded using a force platform. The results showedthat (1) trunk extensor muscles fatigue increased CoP displacements undernormal somatosensatory conditions (Experiment 1 and Experiment 2), (2) thisdestabilizing effect was exacerbated when somatosensation from the foot and theankle was degraded (Experiment 1), and (3) this destabilizing effect wasmitigated when somatosensation from the foot and the ankle was facilitated(Experiment 2). Altogether, the present findings evidenced re-weighting ofsensory cues for controlling posture during quiet standing following trunkextensor muscles fatigue by increasing the reliance on the somatosensory inputsfrom the foot and the ankle. This could have implications in clinical andrehabilitative areas.\tType : Arxiv\t, 14: Titre : Fast B-spline Curve Fitting by L-BFGS\tAuteur : Wenni Zheng, Pengbo Bo, Yang Liu, Wenping Wang\tDate : 2011/12/30\tURL : http://arxiv.org/abs/1201.0070v1\tTexte : We propose a novel method for fitting planar B-spline curves to unorganizeddata points. In traditional methods, optimization of control points and footpoints are performed in two very time-consuming steps in each iteration: 1)control points are updated by setting up and solving a linear system ofequations; and 2) foot points are computed by projecting each data point onto aB-spline curve. Our method uses the L-BFGS optimization method to optimizecontrol points and foot points simultaneously and therefore it does not need toperform either matrix computation or foot point projection in every iteration.As a result, our method is much faster than existing methods.\tType : Arxiv\t, 15: Titre : A control model for zygodactyl bird's foot\tAuteur : Anna Chiara Lai, Paola Loreti\tDate : 2014/04/08\tURL : http://arxiv.org/abs/1404.2072v1\tTexte : In this paper we are interested to the zygodactyly phenomenon in birds, andin particolar in parrots. This arrangement, common in species living on trees,is a distribution of the foot with two toes facing forward and two back. Wegive a model for the foot, and thanks to the methods of iterated functionsystem we are able to describe the reachability set. Moreover we give anecessary and sufficient condition for the grasping problem. Finally weintroduce a hybrid dynamical system modeling owl's foot in various stages ofhunting (flying, attack, grasp).\tType : Arxiv\t, 16: Titre : Diabetic Foot Ulcer Grand Challenge 2022 Summary\tAuteur : Connah Kendrick, Bill Cassidy, Neil D. Reeves, Joseph M. Pappachan, Claire O'Shea, Vishnu Chandrabalan, Moi Hoon Yap\tDate : 2023/04/24\tURL : http://arxiv.org/abs/2304.12001v1\tTexte : The Diabetic Foot Ulcer Challenge 2022 focused on the task of diabetic footulcer segmentation, based on the work completed in previous DFU challenges. Thechallenge provided 4000 images of full-view foot ulcer images together withcorresponding delineation of ulcer regions. This paper provides an overview ofthe challenge, a summary of the methods proposed by the challenge participants,the results obtained from each technique, and a comparison of the challengeresults. The best-performing network was a modified HarDNet-MSEG, with a Dicescore of 0.7287.\tType : Arxiv\t}\n",
      "Score de similarité avec le document A control model for zygodactyl bird's foot : 0.3138145042664165\n",
      "Contenu du document : In this paper we are interested to the zygodactyly phenomenon in birds, andin particolar in parrots. This arrangement, common in species living on trees,is a distribution of the foot with two toes facing forward and two back. Wegive a model for the foot, and thanks to the methods of iterated functionsystem we are able to describe the reachability set. Moreover we give anecessary and sufficient condition for the grasping problem. Finally weintroduce a hybrid dynamical system modeling owl's foot in various stages ofhunting (flying, attack, grasp).\n",
      "\n",
      "\n",
      "Score de similarité avec le document Diabetic Foot Ulcer Grand Challenge 2022 Summary : 0.025526159698363287\n",
      "Contenu du document : The Diabetic Foot Ulcer Challenge 2022 focused on the task of diabetic footulcer segmentation, based on the work completed in previous DFU challenges. Thechallenge provided 4000 images of full-view foot ulcer images together withcorresponding delineation of ulcer regions. This paper provides an overview ofthe challenge, a summary of the methods proposed by the challenge participants,the results obtained from each technique, and a comparison of the challengeresults. The best-performing network was a modified HarDNet-MSEG, with a Dicescore of 0.7287.\n",
      "\n",
      "\n",
      "Score de similarité avec le document Software Based Higher Order Structural Foot Abnormality Detection Using  Image Processing : 0.02313963784636798\n",
      "Contenu du document : The entire movement of human body undergoes through a periodic process namedGait Cycle. The structure of human foot is the key element to complete thecycle successfully. Abnormality of this foot structure is an alarming form ofcongenital disorder which results a classification based on the geometry of thehuman foot print image. Image processing is one of the most efficient way todetermine a number of footprint parameter to detect the severeness of disorder.This paper aims to detect the Flatfoot and High Arch foot abnormalities usingone of the footprint parameters named Modified Brucken Index by biomedicalimage processing.\n",
      "\n",
      "\n",
      "Score de similarité avec le document f2IMU-R: Pedestrian Navigation by Low-cost Foot-Mounted Dual IMUs and  Inter-foot Ranging : 0.020279332390865476\n",
      "Contenu du document : Foot-mounted inertial sensors become popular in many indoor or GPS-deniedapplications, including but not limited to medical monitoring, gait analysis,soldier and first responder positioning. However, the foot-mounted inertialnavigation relies largely on the aid of Zero Velocity Update (ZUPT) and hasencountered inherent problems such as heading drift. This paper implements apedestrian navigation system based on dual foot-mounted low-cost inertialmeasurement units (IMU) and inter-foot ultrasonic ranging. The observabilityanalysis of the system is performed to investigate the roles of the ZUPTmeasurement and the foot-to-foot ranging measurement in improving the stateestimability. A Kalman-based estimation algorithm is mechanized in the Earthframe, rather than in the common local-level frame, which is found to beeffective in depressing the linearization error in Kalman filtering. Anellipsoid constraint in the Earth frame is also proposed to further restrictthe height drift. Simulation and real field experiments show that the proposedmethod has better robustness and positioning accuracy (about 0.1-0.2% travelleddistance) than the traditional pedestrian navigation schemes do.\n",
      "\n",
      "\n",
      "Score de similarité avec le document Fast B-spline Curve Fitting by L-BFGS : 0.0\n",
      "Contenu du document : We propose a novel method for fitting planar B-spline curves to unorganizeddata points. In traditional methods, optimization of control points and footpoints are performed in two very time-consuming steps in each iteration: 1)control points are updated by setting up and solving a linear system ofequations; and 2) foot points are computed by projecting each data point onto aB-spline curve. Our method uses the L-BFGS optimization method to optimizecontrol points and foot points simultaneously and therefore it does not need toperform either matrix computation or foot point projection in every iteration.As a result, our method is much faster than existing methods.\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import ipywidgets as widgets\n",
    "from IPython.display import display\n",
    "import numpy as np\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "\n",
    "#print(vocab)\n",
    "\n",
    "\n",
    "\n",
    "# Fonction pour gérer la recherche avec la similarité cosinus\n",
    "def effectuer_recherche_cosinus(b):\n",
    "    mon_corpus.construire_mat_tfidf()\n",
    "    vocab = mon_corpus.get_vocabulaire()\n",
    "    mots_clefs_utilisateur = champ_recherche_cosinus.value\n",
    "    top_n = int(champ_top_n.value)\n",
    "    vecteur_requete = np.zeros(len(vocab))\n",
    "\n",
    "    mots_clefs_propres = mon_corpus.nettoyer_texte(mots_clefs_utilisateur)\n",
    "    for mot in mots_clefs_propres.split():\n",
    "        if mot in vocab:\n",
    "            mot_id = vocab[mot]['id'] - 1\n",
    "            vecteur_requete[mot_id] += 1\n",
    "\n",
    "    #calcule la similarité6\n",
    "\n",
    "\n",
    "    mat_tfidf = mon_corpus.get_mattdidf()\n",
    "    similarites = cosine_similarity([vecteur_requete], mat_tfidf)\n",
    "\n",
    "    indices_tries = np.argsort(similarites[0])[::-1]\n",
    "    top_n = 5\n",
    "    id2doc = mon_corpus.get_id2doc()\n",
    "    print(id2doc)\n",
    "    for i in range(top_n):\n",
    "        indice_document = indices_tries[i]\n",
    "        score_similarite = similarites[0, indice_document]\n",
    "\n",
    "        # Assurez-vous que l'indice est décalé de 1 pour correspondre à votre indexation\n",
    "        indice_document += 1\n",
    "\n",
    "        document = id2doc[indice_document]\n",
    "\n",
    "        print(f\"Score de similarité avec le document {document.titre} : {score_similarite}\")\n",
    "        print(f\"Contenu du document : {document.texte}\")\n",
    "        print(\"\\n\")\n",
    "\n",
    "# Champ de recherche pour la similarité cosinus\n",
    "champ_recherche_cosinus = widgets.Text(description=\"Mots-clés:\", placeholder=\"Entrez vos mots-clés\")\n",
    "\n",
    "# Champ pour spécifier le nombre de résultats à afficher\n",
    "champ_top_n = widgets.IntText(description=\"Top N:\", value=5)\n",
    "\n",
    "# Bouton pour effectuer la recherche avec la similarité cosinus\n",
    "bouton_recherche_cosinus = widgets.Button(description=\"Rechercher avec Cosinus\")\n",
    "bouton_recherche_cosinus.on_click(effectuer_recherche_cosinus)\n",
    "\n",
    "# Afficher le formulaire\n",
    "display(champ_recherche_cosinus, champ_top_n, bouton_recherche_cosinus)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
